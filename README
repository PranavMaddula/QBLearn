Data:
---------------------------------------------------------------------------------------------------
    The history dataset contains 1,713 questions (8,149 sentences) and 
    the literature dataset contains 2,246 questions (10,552 sentences). These questions (and
    questions from other categories) are publically available at:
    https://code.google.com/p/protobowl/downloads/detail?name=shuffled.json

        Sample question:
            This author describes the last Neanderthals being killed by Homo sapiens in his The 
            Inheritors, and a lone British naval officer hallucinates survival on a barren rock
            in his Pincher Martin. In another of his novels, the title entity speaks to Simon 
            atop a wooden stick; later in that novel, the followers of Jack steal Piggy's 
            glasses and break the conch shell of Ralph, their former chief. For 10 points, 
            name this British author who described boys on a formerly deserted island in his 
            Lord of the Flies.

        A: William Golding


How to train?
---------------------------------------------------------------------------------------------------
    - To train a model on the history dataset with default parameters, use the following command:
        python qblearn.py

    - To train a model on the literature dataset, use this command:
        python qblearn.py -data 'data/lit_split' -We 'data/lit_We' -b 341 -o 'models/lit_params'

    - For detailed explanation of the hyperparameters, use the help command:
        python qblearn.py -h

    - You can get better performance to a point (along with a longer training time) by increasing 
      the number of training epochs. For the results reported in the sample I ran the script for
      50 epochs. You can also train faster by increasing the number of cores used (default=6,
      change as per your machine's specs).

    - The resulting model is packaged and placed into the models directory.


How to evaluate
---------------------------------------------------------------------------------------------------
    - Use the evaluate script (see following example):
        python qb_learn_eval.py -data data/hist_split -model models/hist_params

    - This script will train a model on QBLearn's learned representations and output both overall 
      accuracy as well as accuracy at each sentence position of the question. In addition, it
      will train BoW and BoW-DT baseline models (as described in the paper) for comparison.

    - As a point of comparison, I get around 60% accuracy on the history dataset and 54% accuracy on 
      literature with QBLearn, which beats the BoW-DT baseline by 10-15% and the BoW baseline by ~20%.
      This shows that QBLearn is still effective on smaller datasets.


Own Data?
---------------------------------------------------------------------------------------------------
    - A preprocessing script has been provided to dependency parse your own questions and 
      convert them into the tree format required by QBLearn. You'll need to download the Stanford parser
      (http://nlp.stanford.edu/software/lex-parser.shtml) to run the scripts. Also, a modified version
      of the parsing script lexparser.sh has been provided to output only typed dependencies.


 Acknowledgements
---------------------------------------------------------------------------------------------------
Work is implementaion of https://people.cs.umass.edu/~miyyer/pubs/2014_qb_rnn.pdf
originally published by Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daum√© III